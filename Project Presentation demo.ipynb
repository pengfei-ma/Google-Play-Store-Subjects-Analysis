{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Finding Simple Linear Regression Equation #####\n",
      "The m (coefficient) = [0.00285053]\n",
      "The b (y-intercept) = 0.3456192319738405\n",
      "The equation is: y = 0.0028505272834074033X + 0.3456192319738405\n",
      "##### Finding the parameters using gradient descent #####\n",
      "0 Beta [0.1]  Cost [1.22465805e+09]\n",
      "1 Beta [-0.0045326]  Cost [12179629.77029447]\n",
      "2 Beta [0.00420567]  Cost [3706927.27275984]\n",
      "3 Beta [0.00347521]  Cost [3647720.70333904]\n",
      "4 Beta [0.00353627]  Cost [3647306.97255329]\n",
      "5 Beta [0.00353116]  Cost [3647304.08143576]\n",
      "6 Beta [0.00353159]  Cost [3647304.06123297]\n",
      "7 Beta [0.00353156]  Cost [3647304.06109216]\n",
      "8 Beta [0.00353156]  Cost [3647304.06109071]\n",
      "9 Beta [0.00353156]  Cost [3647304.06109042]\n",
      "10 Beta [0.00353156]  Cost [3647304.06109096]\n",
      "11 Beta [0.00353156]  Cost [3647304.06109107]\n",
      "12 Beta [0.00353156]  Cost [3647304.06109113]\n",
      "13 Beta [0.00353156]  Cost [3647304.06109092]\n",
      "14 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "15 Beta [0.00353156]  Cost [3647304.06109096]\n",
      "16 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "17 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "18 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "19 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "20 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "21 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "22 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "23 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "24 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "25 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "26 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "27 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "28 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "29 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "30 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "31 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "32 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "33 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "34 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "35 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "36 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "37 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "38 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "39 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "40 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "41 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "42 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "43 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "44 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "45 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "46 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "47 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "48 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "49 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "50 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "51 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "52 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "53 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "54 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "55 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "56 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "57 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "58 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "59 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "60 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "61 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "62 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "63 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "64 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "65 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "66 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "67 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "68 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "69 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "70 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "71 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "72 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "73 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "74 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "75 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "76 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "77 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "78 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "79 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "80 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "81 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "82 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "83 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "84 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "85 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "86 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "87 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "88 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "89 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "90 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "91 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "92 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "93 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "94 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "95 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "96 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "97 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "98 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "99 Beta [0.00353156]  Cost [3647304.06109097]\n",
      "Computation time of BGD is 38.446050798892976 Minutes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Finding the parameters of multi-linear regression using gradient descent #####\n",
      "0 Beta [0.1 0.1 0.1 0.1]  Cost 5.0307575633564e+18\n",
      "1 Beta [ 1.00000561e-01 -6.52332938e+03  1.00001340e-01 -4.33113272e+07]  Cost 9.435631463641919e+35\n",
      "2 Beta [1.00001122e-01 4.25538269e+08 1.00002679e-01 1.87587107e+16]  Cost 1.7700028199286235e+53\n",
      "3 Beta [ 1.00001683e-01 -2.77592634e+13  1.00004019e-01 -8.12464663e+24]  Cost 3.3202971042668924e+70\n",
      "4 Beta [1.00002245e-01 1.81082821e+18 1.00005359e-01 3.51889230e+33]  Cost 6.228449320237621e+87\n",
      "5 Beta [ 1.00002806e-01 -1.18126291e+23  1.00006698e-01 -1.52407897e+42]  Cost 1.1683767963088338e+105\n",
      "6 Beta [1.00003367e-01 7.70576714e+27 1.00008038e-01 6.60098831e+50]  Cost 2.1917242446163375e+122\n",
      "7 Beta [ 1.00003928e-01 -5.02672580e+32  1.00009378e-01 -2.85897566e+59]  Cost 4.111392129332666e+139\n",
      "8 Beta [1.00004489e-01 3.27909886e+37 1.00010717e-01 1.23826031e+68]  Cost 7.712441600561638e+156\n",
      "9 Beta [ 1.00005050e-01 -2.13906423e+42  1.00012057e-01 -5.36306976e+76]  Cost 1.4467546167076145e+174\n",
      "10 Beta [1.00005611e-01 1.39538207e+47 1.00013396e-01 2.32281670e+85]  Cost 2.7139251476631882e+191\n",
      "11 Beta [ 1.00006172e-01 -9.10253702e+51  1.00014736e-01 -1.00604275e+94]  Cost 5.090973702147291e+208\n",
      "12 Beta [1.00006734e-001 5.93788481e+056 1.00016076e-001 4.35730468e+102]  Cost 9.550010345079675e+225\n",
      "13 Beta [ 1.00007295e-001 -3.87347791e+061  1.00017415e-001 -1.88720650e+111]  Cost 1.791458823538236e+243\n",
      "14 Beta [1.00007856e-001 2.52679726e+066 1.00018755e-001 8.17374184e+119]  Cost 3.360545800965015e+260\n",
      "15 Beta [ 1.00008417e-001 -1.64831311e+071  1.00020095e-001 -3.54015609e+128]  Cost 6.303950686446002e+277\n",
      "16 Beta [1.00008978e-001 1.07524895e+076 1.00021434e-001 1.53328859e+137]  Cost 1.1825398792580455e+295\n",
      "17 Beta [ 1.00009539e-001 -7.01420317e+080  1.00022774e-001 -6.64087643e+145]  Cost inf\n",
      "18 Beta [1.00010100e-001 4.57559584e+085 1.00024113e-001 2.87625173e+154]  Cost inf\n",
      "19 Beta [ 1.00010661e-001 -2.98481192e+090  1.00025453e-001 -1.24574281e+163]  Cost inf\n",
      "20 Beta [1.00011222e-001 1.94709116e+095 1.00026793e-001 5.39547745e+171]  Cost inf\n",
      "21 Beta [ 1.00011783e-001 -1.27015172e+100  1.00028132e-001 -2.33685290e+180]  Cost inf\n",
      "22 Beta [1.00012345e-001 8.28561815e+104 1.00029472e-001 1.01212201e+189]  Cost inf\n",
      "23 Beta [ 1.00012906e-001 -5.40498172e+109  1.00030812e-001 -4.38363479e+197]  Cost inf\n",
      "24 Beta [1.00013467e-001 3.52584766e+114 1.00032151e-001 1.89861041e+206]  Cost inf\n",
      "25 Beta [ 1.00014028e-001 -2.30002660e+119  1.00033491e-001 -8.22313373e+214]  Cost inf\n",
      "26 Beta [1.00014589e-001 1.50038313e+124 1.00034830e-001 3.56154837e+223]  Cost inf\n",
      "27 Beta [ 1.00015150e-001 -9.78749352e+128  1.00036170e-001 -1.54255388e+232]  Cost inf\n",
      "28 Beta [1.00015711e-001 6.38470450e+133 1.00037510e-001 6.68100560e+240]  Cost inf\n",
      "29 Beta [ 1.00016272e-001 -4.16495311e+138  1.00038849e-001 -2.89363221e+249]  Cost inf\n",
      "30 Beta [1.00016833e-001 2.71693614e+143 1.00040189e-001 1.25327052e+258]  Cost inf\n",
      "31 Beta [ 1.00017394e-001 -1.77234697e+148  1.00041528e-001 -5.42808098e+266]  Cost inf\n",
      "32 Beta [1.00017955e-001 1.15616032e+153 1.00042868e-001 2.35097392e+275]  Cost inf\n",
      "33 Beta [ 1.00018516e-001 -7.54201471e+157  1.00044208e-001 -1.01823801e+284]  Cost inf\n",
      "34 Beta [1.00019077e-001 4.91990469e+162 1.00045547e-001 4.41012400e+292]  Cost inf\n",
      "35 Beta [ 1.00019638e-001 -3.20941593e+167  1.00046887e-001             -inf]  Cost nan\n",
      "36 Beta [1.00020199e-001 2.09360776e+172 1.00048226e-001             nan]  Cost nan\n",
      "37 Beta [ 1.00020761e-001 -1.36572932e+177  1.00049566e-001              nan]  Cost nan\n",
      "38 Beta [1.00021322e-001 8.90910235e+181 1.00050906e-001             nan]  Cost nan\n",
      "39 Beta [ 1.00021883e-001 -5.81170100e+186  1.00052245e-001              nan]  Cost nan\n",
      "40 Beta [1.00022444e-001 3.79116405e+191 1.00053585e-001             nan]  Cost nan\n",
      "41 Beta [ 1.00023005e-001 -2.47310122e+196  1.00054924e-001              nan]  Cost nan\n",
      "42 Beta [1.00023566e-001 1.61328541e+201 1.00056264e-001             nan]  Cost nan\n",
      "43 Beta [ 1.00024127e-001 -1.05239923e+206  1.00057603e-001              nan]  Cost nan\n",
      "44 Beta [1.00024688e-001 6.86514692e+210 1.00058943e-001             nan]  Cost nan\n",
      "45 Beta [ 1.00025249e-001 -4.47836153e+215  1.00060283e-001              nan]  Cost nan\n",
      "46 Beta [1.00025810e-001 2.92138278e+220 1.00061622e-001             nan]  Cost nan\n",
      "47 Beta [ 1.00026371e-001 -1.90571424e+225  1.00062962e-001              nan]  Cost nan\n",
      "48 Beta [1.00026932e-001 1.24316019e+230 1.00064301e-001             nan]  Cost nan\n",
      "49 Beta [ 1.00027493e-001 -8.10954352e+234  1.00065641e-001              nan]  Cost nan\n",
      "50 Beta [1.00028054e-001 5.29012243e+239 1.00066980e-001             nan]  Cost nan\n",
      "51 Beta [ 1.00028615e-001 -3.45092116e+244  1.00068320e-001              nan]  Cost nan\n",
      "52 Beta [1.00029176e-001 2.25114958e+249 1.00069659e-001             nan]  Cost nan\n",
      "53 Beta [ 1.00029737e-001 -1.46849904e+254  1.00070999e-001              nan]  Cost nan\n",
      "54 Beta [1.00030298e-001 9.57950309e+258 1.00072339e-001             nan]  Cost nan\n",
      "55 Beta [ 1.00030859e-001 -6.24902549e+263  1.00073678e-001              nan]  Cost nan\n",
      "56 Beta [1.00031420e-001 4.07644522e+268 1.00075018e-001             nan]  Cost nan\n",
      "57 Beta [ 1.00031981e-001 -2.65919953e+273  1.00076357e-001              nan]  Cost nan\n",
      "58 Beta [1.00032542e-001 1.73468347e+278 1.00077697e-001             nan]  Cost nan\n",
      "59 Beta [ 1.00033103e-001 -1.13159118e+283  1.00079036e-001              nan]  Cost nan\n",
      "60 Beta [1.00033664e-001 7.38174212e+287 1.00080376e-001             nan]  Cost nan\n",
      "61 Beta [ 1.00034225e-001 -4.81535360e+292  1.00081715e-001              nan]  Cost nan\n",
      "62 Beta [0.10003479        inf 0.10008305        nan]  Cost nan\n",
      "63 Beta [0.10003535        nan 0.10008439        nan]  Cost nan\n",
      "64 Beta [0.10003591        nan 0.10008573        nan]  Cost nan\n",
      "65 Beta [0.10003647        nan 0.10008707        nan]  Cost nan\n",
      "66 Beta [0.10003703        nan 0.10008841        nan]  Cost nan\n",
      "67 Beta [0.10003759        nan 0.10008975        nan]  Cost nan\n",
      "68 Beta [0.10003815        nan 0.10009109        nan]  Cost nan\n",
      "69 Beta [0.10003871        nan 0.10009243        nan]  Cost nan\n",
      "70 Beta [0.10003927        nan 0.10009377        nan]  Cost nan\n",
      "71 Beta [0.10003983        nan 0.10009511        nan]  Cost nan\n",
      "72 Beta [0.1000404         nan 0.10009645        nan]  Cost nan\n",
      "73 Beta [0.10004096        nan 0.10009779        nan]  Cost nan\n",
      "74 Beta [0.10004152        nan 0.10009913        nan]  Cost nan\n",
      "75 Beta [0.10004208        nan 0.10010047        nan]  Cost nan\n",
      "76 Beta [0.10004264        nan 0.10010181        nan]  Cost nan\n",
      "77 Beta [0.1000432         nan 0.10010315        nan]  Cost nan\n",
      "78 Beta [0.10004376        nan 0.10010449        nan]  Cost nan\n",
      "79 Beta [0.10004432        nan 0.10010583        nan]  Cost nan\n",
      "80 Beta [0.10004488        nan 0.10010717        nan]  Cost nan\n",
      "81 Beta [0.10004544        nan 0.10010851        nan]  Cost nan\n",
      "82 Beta [0.10004601        nan 0.10010985        nan]  Cost nan\n",
      "83 Beta [0.10004657        nan 0.10011118        nan]  Cost nan\n",
      "84 Beta [0.10004713        nan 0.10011252        nan]  Cost nan\n",
      "85 Beta [0.10004769        nan 0.10011386        nan]  Cost nan\n",
      "86 Beta [0.10004825        nan 0.1001152         nan]  Cost nan\n",
      "87 Beta [0.10004881        nan 0.10011654        nan]  Cost nan\n",
      "88 Beta [0.10004937        nan 0.10011788        nan]  Cost nan\n",
      "89 Beta [0.10004993        nan 0.10011922        nan]  Cost nan\n",
      "90 Beta [0.10005049        nan 0.10012056        nan]  Cost nan\n",
      "91 Beta [0.10005105        nan 0.1001219         nan]  Cost nan\n",
      "92 Beta [0.10005161        nan 0.10012324        nan]  Cost nan\n",
      "93 Beta [0.10005218        nan 0.10012458        nan]  Cost nan\n",
      "94 Beta [0.10005274        nan 0.10012592        nan]  Cost nan\n",
      "95 Beta [0.1000533         nan 0.10012726        nan]  Cost nan\n",
      "96 Beta [0.10005386        nan 0.1001286         nan]  Cost nan\n",
      "97 Beta [0.10005442        nan 0.10012994        nan]  Cost nan\n",
      "98 Beta [0.10005498        nan 0.10013128        nan]  Cost nan\n",
      "99 Beta [0.10005554        nan 0.10013262        nan]  Cost nan\n",
      "Computation time of multi-linear regression by BGD is 24.002660648028055 minutes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Logistic regression model #####\n",
      "Acc of Test:  0.9975432599871822\n",
      "F1 of Test: 0.9974863907576629\n",
      "Computation time of logistic regression is 28.70752596855166 minutes\n",
      "##### SVM model #####\n",
      "Acc of Test:  0.982736100123734\n",
      "F1 of Test: 0.980016837261283\n",
      "Computation time of logistic regression is 29.631058894040428 minutes\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from operator import add\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "import pyspark\n",
    "from pyspark.ml.linalg import Vectors\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql import SQLContext\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "from pandas import Series,DataFrame\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "\n",
    "# building functions\n",
    "\n",
    "def isfloat(value):\n",
    "    try:\n",
    "        float(value)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def correctRows(p):\n",
    "    if isfloat(p[3]) and isfloat(p[4]) and isfloat(p[6]) and isfloat(p[7]) and isfloat(p[9]):\n",
    "        return p\n",
    "    \n",
    "def to_list(a):\n",
    "    return [a]\n",
    "\n",
    "def addToList(x, y):\n",
    "    x.append(y)\n",
    "    return x\n",
    "\n",
    "def extend(x,y):\n",
    "    x.extend(y)\n",
    "    return x\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage: wordcount <file> <output> \", file=sys.stderr)\n",
    "        exit(-1)\n",
    "\n",
    "    spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    sqlContext = SQLContext(sc)\n",
    "    \n",
    "    # load data set\n",
    "    lines2 = sc.textFile(\"Google-Playstore.csv\")\n",
    "    \n",
    "    # generate test case file\n",
    "    df = pd.read_csv(\"Google-Playstore.csv\")\n",
    "    test_case = df.sample(n = 10000)\n",
    "    test_case.to_csv('Google-Playstore_test.csv', index=False)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Simple Linear Regression\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"##### Finding Simple Linear Regression Equation #####\")\n",
    "    \n",
    "    # data pre-processing\n",
    "    \n",
    "    correctLine = lines2.map(lambda x: x.split(','))\n",
    "    cleaned = correctLine.filter(correctRows)\n",
    "    \n",
    "    max_install = cleaned.map(lambda p: (float(p[7])))\n",
    "    rating = cleaned.map(lambda p: (float(p[3])))\n",
    "    \n",
    "    # apply linear regression\n",
    "    x = np.array(max_install.collect())\n",
    "    y = np.array(rating.collect())\n",
    "\n",
    "    X = np.stack([x], axis = 1)\n",
    "\n",
    "    reg = LinearRegression(fit_intercept=True).fit(X, y)\n",
    "    \n",
    "    print(\"The m (coefficient) =\",reg.coef_)\n",
    "    print(\"The b (y-intercept) =\",reg.intercept_)\n",
    "    print(\"The equation is: y = \"+str(reg.coef_[0])+\"X + \"+str(reg.intercept_))\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Gradient Descent for parameters\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"##### Finding the parameters using gradient descent #####\")\n",
    "    \n",
    "    start1 = time.time()\n",
    "    df = np.stack([y, x], axis=1)\n",
    "    dff = map(lambda x: (float(x[0]), Vectors.dense(x[1:])), df)\n",
    "    mydf = spark.createDataFrame(dff, schema=[\"Money\", \"Distance\"])\n",
    "    myRDD=mydf.rdd.map(tuple).map(lambda x: (float(x[0]), np.array(x[1]) ))\n",
    "\n",
    "    learningRate = 0.00001\n",
    "    num_iteration = 100\n",
    "    size = float(len(y))\n",
    "    beta = np.array([0.1])\n",
    "    costs = []\n",
    "\n",
    "    for i in range(num_iteration):\n",
    "        gradientCost=myRDD.map(lambda x: (x[1], (x[0] - x[1] * beta) ))\\\n",
    "                               .map(lambda x: (x[0]*x[1], x[1]**2 )).reduce(lambda x, y: (x[0] +y[0], x[1]+y[1] ))\n",
    "        cost= gradientCost[1]\n",
    "        gradient=(-1/float(size))* gradientCost[0]\n",
    "        print(i, \"Beta\", beta, \" Cost\", cost)\n",
    "        beta = beta - learningRate * gradient\n",
    "        costs.append(cost[0])\n",
    "\n",
    "    end1 = time.time()\n",
    "\n",
    "    print(f\"Computation time of BGD is {(end1 - start1)/60} Minutes\")\n",
    "    \n",
    "    # making plot\n",
    "    xValues = [i for i in range(len(costs))]\n",
    "    plt.plot(xValues, costs, 'o', markersize=2)\n",
    "    plt.xlabel(\"Number of Iteration\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.title(\"Cost with the number of iteration\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Multi-Linear Regression\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"##### Finding the parameters of multi-linear regression using gradient descent #####\")\n",
    "    \n",
    "    start2 = time.time()\n",
    "\n",
    "    rating = cleaned.map(lambda p: (p[0], p[3]))\n",
    "    rating_count = cleaned.map(lambda p: (p[0], p[4]))\n",
    "    min_install = cleaned.map(lambda p: (p[0], p[6]))\n",
    "    max_install = cleaned.map(lambda p: (p[0], p[7]))\n",
    "    price = cleaned.map(lambda p: (p[0], p[9]))\n",
    "\n",
    "    rating = rating.combineByKey(to_list, addToList, extend)\n",
    "    rating = rating.collect()\n",
    "    rating_count = rating_count.combineByKey(to_list, addToList, extend)\n",
    "    rating_count = rating_count.collect()\n",
    "    min_install = min_install.combineByKey(to_list, addToList, extend)\n",
    "    min_install = min_install.collect()\n",
    "    max_install = max_install.combineByKey(to_list, addToList, extend)\n",
    "    max_install = max_install.collect()\n",
    "    price = price.combineByKey(to_list, addToList, extend)\n",
    "    price = price.collect()\n",
    "\n",
    "\n",
    "    ratingKey = []\n",
    "    ratingValue = []\n",
    "    for i in range(len(rating)):\n",
    "        ratingKey.append(rating[i][0])\n",
    "\n",
    "        rate = 0\n",
    "        total = 0\n",
    "        for j in [float(i) for i in rating[i][1]]:\n",
    "            rate += j\n",
    "            total += 1\n",
    "\n",
    "        ratingValue.append(rate/total)\n",
    "\n",
    "\n",
    "    ratingCountKey = []\n",
    "    ratingCountValue = []\n",
    "    for i in range(len(rating_count)):\n",
    "        ratingCountKey.append(rating_count[i][0])\n",
    "\n",
    "        rate = 0\n",
    "        for j in [float(i) for i in rating_count[i][1]]:\n",
    "            rate += j\n",
    "\n",
    "        ratingCountValue.append(rate)\n",
    "\n",
    "\n",
    "    min_installKey = []\n",
    "    min_installValue = []\n",
    "    for i in range(len(min_install)):\n",
    "        min_installKey.append(min_install[i][0])\n",
    "\n",
    "        count = 0\n",
    "        for j in [float(i) for i in min_install[i][1]]:\n",
    "            if j != 0:\n",
    "                count += 1\n",
    "\n",
    "        min_installValue.append(count)\n",
    "\n",
    "\n",
    "    max_installKey = []\n",
    "    max_installValue = []\n",
    "    for i in range(len(max_install)):\n",
    "        min_installKey.append(max_install[i][0])\n",
    "\n",
    "        count = 0\n",
    "        for j in [float(i) for i in max_install[i][1]]:\n",
    "            if j != 0:\n",
    "                count += 1\n",
    "\n",
    "        max_installValue.append(count)\n",
    "\n",
    "\n",
    "    priceKey = []\n",
    "    priceValue = []\n",
    "    for i in range(len(price)):\n",
    "        priceKey.append(price[i][0])\n",
    "\n",
    "        amount = 0\n",
    "        for j in [float(i) for i in price[i][1]]:\n",
    "            amount += j\n",
    "\n",
    "        priceValue.append(amount)\n",
    "\n",
    "\n",
    "    app = ratingKey\n",
    "    rating = ratingValue\n",
    "    countOfRating = ratingCountValue\n",
    "    mi_install = min_installValue\n",
    "    Price = priceValue\n",
    "\n",
    "    ma_install = max_installValue\n",
    "\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for i in range(len(app)):\n",
    "        x.append([float(rating[i]), float(countOfRating[i]), float(mi_install[i]), float(Price[i])])\n",
    "        y.append(float(ma_install[i]))\n",
    "\n",
    "    learningRate = 0.000001\n",
    "    num_iteration = 100\n",
    "    size = len(y)\n",
    "    costs = []\n",
    "    beta = np.array([0.1, 0.1, 0.1, 0.1])\n",
    "\n",
    "    data = {'y':y, 'x':x}\n",
    "    df = DataFrame(data)\n",
    "    spark_df_from_pandas = spark.createDataFrame(df, schema=['x', 'y'])\n",
    "    myRDD=spark_df_from_pandas.rdd.map(lambda x: (float(x[0]), np.array(x[1])))\n",
    "\n",
    "    for i in range(num_iteration):\n",
    "        gradientCost=myRDD.map(lambda x: (x[1], (x[0] - x[1] * beta)))\\\n",
    "                               .map(lambda x: (x[0]*x[1], x[1]**2 )).reduce(lambda x, y: (x[0] +y[0], x[1]+y[1] ))\n",
    "\n",
    "        cost = 0\n",
    "        for j in gradientCost[1]:\n",
    "            cost += j\n",
    "\n",
    "        gradient=(-1/float(size))* gradientCost[0]\n",
    "        print(i, \"Beta\", beta, \" Cost\", cost)\n",
    "        beta = beta - learningRate * gradient\n",
    "\n",
    "        costs.append(cost)\n",
    "\n",
    "    end2 = time.time()\n",
    "\n",
    "    print(f\"Computation time of multi-linear regression by BGD is {(end2 - start2)/60} minutes\")\n",
    "\n",
    "    xValues = [i for i in range(len(costs))]\n",
    "\n",
    "    plt.plot(xValues, costs, 'o', markersize=2)\n",
    "    plt.xlabel(\"Number of Iteration\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.title(\"Cost with the number of iteration\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Logistic Regression\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    print(\"##### gradient descent algorithm to learn a logistic regression model #####\")\n",
    "    \n",
    "    start3 = time.time()\n",
    "    \n",
    "    total_install = cleaned.map(lambda p: (p[0], p[7]))\n",
    "    tuples = total_install.collect()\n",
    "\n",
    "    appWords = []\n",
    "\n",
    "    for i in app:\n",
    "        words = i.split(\" \")\n",
    "        for j in words:\n",
    "            j = re.sub('[^A-Za-z0-9]+', '', j)\n",
    "            appWords.append(j)\n",
    "\n",
    "    appWords = ' '.join(appWords).split()\n",
    "\n",
    "    allWords = sc.parallelize(appWords)\n",
    "    allCount = allWords.map(lambda x: (x, 1)).reduceByKey(add)\n",
    "    topWords = allCount.top(20000, lambda x: x[1])\n",
    "\n",
    "    topWordsK = sc.parallelize(range(20000))\n",
    "    dictionary = topWordsK.map(lambda x: (topWords[x][0], x))\n",
    "\n",
    "    def TF(words_list, top_words):\n",
    "        words_dict = dict(Counter(words_list))\n",
    "        tf_vector = []\n",
    "        for word in top_words:\n",
    "            if word in words_dict.keys():\n",
    "                tf = words_dict[word]\n",
    "                tf_vector.append(tf)\n",
    "            else:\n",
    "                tf_vector.append(0)\n",
    "        return tf_vector\n",
    "\n",
    "    key_id = [i for i in range(len(tuples))]\n",
    "    key_values = {tuples[i][0]: i for i in key_id}\n",
    "    topWordsBC = sc.broadcast(dictionary.keys().collect())\n",
    "    feat = cleaned.map(lambda x: (key_values[x[0]], TF(x[1], topWordsBC.value)))\n",
    "    labels = cleaned.map(lambda x: (key_values[x[0]], int(x[0][0] == 'A' and x[0][1] == 'U')))\n",
    "    trainRDD = feat.join(labels)\n",
    "\n",
    "    learningRate = 0.0003\n",
    "    num_iteration = 5\n",
    "    lambda_cof = 0.01\n",
    "    size = len(tuples)\n",
    "\n",
    "    loss_list = list()\n",
    "\n",
    "    parameter_vector = np.random.normal(0, 0.1, (dictionary.count(), 1))\n",
    "    \n",
    "    def sigmoid(x):\n",
    "        return 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "    def loss_func(feat_line, y, parameter_vector):\n",
    "        feat_line = np.array(feat_line)\n",
    "        pred = sigmoid(np.dot(feat_line, parameter_vector))\n",
    "        return -y * np.log(pred + 1e-12) - (1 - y) * np.log((1 - pred) + 1e-12)\n",
    "\n",
    "    def accuracy_score(feat_line, y, parameter_vector):\n",
    "        feat_line = np.array(feat_line)\n",
    "        pred = sigmoid(np.dot(feat_line, parameter_vector))\n",
    "        pred = 1 if pred >= 0.5 else 0\n",
    "        acc = int(pred == y)\n",
    "        return acc\n",
    "\n",
    "    def grad_func(feat_line, y, parameter_vector):\n",
    "        feat_line = np.array(feat_line)\n",
    "        pred = sigmoid(np.dot(feat_line, parameter_vector))\n",
    "        grad = (pred - y) @ feat_line[None, :]\n",
    "        return grad\n",
    "\n",
    "    acc_his = []\n",
    "    loss_his = []\n",
    "    grad_his = []\n",
    "    prev_param_norm = 0\n",
    "    \n",
    "    for i in tqdm.trange(num_iteration):\n",
    "        parameter_vector_BC = sc.broadcast(parameter_vector)\n",
    "\n",
    "        loss = trainRDD.map(lambda x: loss_func(x[1][0], x[1][1], parameter_vector_BC.value)).reduce(add) / size\n",
    "        acc = trainRDD.map(lambda x: accuracy_score(x[1][0], x[1][1], parameter_vector_BC.value)).reduce(add) / size\n",
    "        grad = trainRDD.map(lambda x: grad_func(x[1][0], x[1][1], parameter_vector_BC.value)).reduce(add) / size\n",
    "\n",
    "        parameter_vector = parameter_vector - learningRate * grad[:, None]\n",
    "\n",
    "        if np.abs(np.linalg.norm(parameter_vector) - prev_param_norm) < 1e-7:\n",
    "            print('Break')\n",
    "            break\n",
    "\n",
    "        prev_param_norm = np.linalg.norm(parameter_vector)\n",
    "        # L2\n",
    "        parameter_vector = parameter_vector - 2 * lambda_cof * parameter_vector\n",
    "\n",
    "        acc_his.append(acc)\n",
    "        loss_his.append(loss)\n",
    "        grad_his.append(np.linalg.norm(grad))\n",
    "    \n",
    "    end3 = time.time()\n",
    "    \n",
    "    print(f\"Computation time of multi-linear regression by BGD is {(end3 - start3)/60} minutes\")\n",
    "\n",
    "    fig, ax = plt.subplots(3, figsize=(13, 13))\n",
    "    ax[0].set_title('Accurary')\n",
    "    ax[0].plot(acc_his)\n",
    "    ax[1].set_title('Loss')\n",
    "    ax[1].plot(loss_his)\n",
    "    ax[2].set_title('GradNorm')\n",
    "    ax[2].plot(grad_his)\n",
    "    plt.savefig(\"TrainingProcess.png\")\n",
    "    plt.show()\n",
    "\n",
    "    print('The five words with the largest coefficients',\n",
    "          np.array(topWordsBC.value)[np.argsort(parameter_vector[:, 0])[-5:]])\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    Logistic Regression Model Evaluation\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    print(\"##### model evaluation #####\")\n",
    "    \n",
    "    start4 = time.time()\n",
    "    \n",
    "    t_tuples = total_install.collect()\n",
    "    key_id = [i for i in range(len(tuples))]\n",
    "    key_values = {tuples[i][0]: i for i in key_id}\n",
    "    topWordsBC = sc.broadcast(dictionary.keys().collect())\n",
    "    feat = cleaned.map(lambda x: (key_values[x[0]], TF(x[1], topWordsBC.value)))\n",
    "    labels = cleaned.map(lambda x: (key_values[x[0]], int(x[0][0] == 'A' and x[0][1] == 'U')))\n",
    "    testRDD = feat.join(labels)\n",
    "\n",
    "\n",
    "    # val\n",
    "    def TP_func(feat_line, y, parameter_vector):\n",
    "        feat_line = np.array(feat_line)\n",
    "        pred = sigmoid(np.dot(feat_line, parameter_vector))\n",
    "        pred = 1 if pred >= 0.5 else 0\n",
    "        TP = int(pred == 1 and y == 1)\n",
    "        return TP\n",
    "\n",
    "\n",
    "    def FP_func(feat_line, y, parameter_vector):\n",
    "        feat_line = np.array(feat_line)\n",
    "        pred = sigmoid(np.dot(feat_line, parameter_vector))\n",
    "        pred = 1 if pred >= 0.5 else 0\n",
    "        FP = int(pred == 1 and y != 1)\n",
    "        return FP\n",
    "\n",
    "\n",
    "    def FN_func(feat_line, y, parameter_vector):\n",
    "        feat_line = np.array(feat_line)\n",
    "        pred = sigmoid(np.dot(feat_line, parameter_vector))\n",
    "        pred = 1 if pred >= 0.5 else 0\n",
    "        FN = int(pred != 1 and y == 1)\n",
    "        return FN\n",
    "\n",
    "\n",
    "    def TN_func(feat_line, y, parameter_vector):\n",
    "        feat_line = np.array(feat_line)\n",
    "        pred = sigmoid(np.dot(feat_line, parameter_vector))\n",
    "        pred = 1 if pred >= 0.5 else 0\n",
    "        TN = int(pred != 1 and y != 1)\n",
    "        return TN\n",
    "\n",
    "\n",
    "    parameter_vector_BC = sc.broadcast(parameter_vector)\n",
    "    acc = testRDD.map(lambda x: accuracy_score(x[1][0], x[1][1], parameter_vector_BC.value)).reduce(add) / size\n",
    "    TP = testRDD.map(lambda x: TP_func(x[1][0], x[1][1], parameter_vector_BC.value)).reduce(add)\n",
    "    FP = testRDD.map(lambda x: FP_func(x[1][0], x[1][1], parameter_vector_BC.value)).reduce(add)\n",
    "    FN = testRDD.map(lambda x: FN_func(x[1][0], x[1][1], parameter_vector_BC.value)).reduce(add)\n",
    "    TN = testRDD.map(lambda x: TN_func(x[1][0], x[1][1], parameter_vector_BC.value)).reduce(add)\n",
    "\n",
    "    F1 = 2 * TP / (2 * TP + FN + FP)\n",
    "    print('The Acc of Test: ', acc)\n",
    "    print('The F1 score of Test: ', F1)\n",
    "    \n",
    "    print(f\"Computation time of multi-linear regression by BGD is {(end4 - start4)/60} minutes\")\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    SVM Model\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    print(\"##### SVM model #####\")\n",
    "    \n",
    "    start5 = time.time()\n",
    "    \n",
    "    d_corpus = sc.textFile(\"Google-Playstore.csv\")\n",
    "    d_keyAndText = d_corpus.map(lambda x: (x[x.index('id=\"') + 4: x.index('\" url=')], x[x.index('\">') + 2:][:-6]))\n",
    "    regex = re.compile('[^a-zA-Z]')\n",
    "\n",
    "    d_keyAndListOfWords = d_keyAndText.map(lambda x: (str(x[0]), regex.sub(' ', x[1]).lower().split())).sortByKey(False)\n",
    "\n",
    "    tuples = d_keyAndListOfWords.collect()\n",
    "    allWordsList = []\n",
    "\n",
    "    for i in range(len(tuples)):\n",
    "        for j in tuples[i][1]:\n",
    "            allWordsList.append(j)\n",
    "\n",
    "    allWords = sc.parallelize(allWordsList)\n",
    "    allCount = allWords.map(lambda x: (x, 1)).reduceByKey(add)\n",
    "    topWords = allCount.top(20000, lambda x: x[1])\n",
    "\n",
    "    topWordsK = sc.parallelize(range(20000))\n",
    "    dictionary = topWordsK.map(lambda x: (topWords[x][0], x))\n",
    "\n",
    "\n",
    "    def TF(words_list, top_words):\n",
    "        words_dict = dict(Counter(words_list))\n",
    "        tf_vector = []\n",
    "        for word in top_words:\n",
    "            if word in words_dict.keys():\n",
    "                tf = words_dict[word]\n",
    "                tf_vector.append(tf)\n",
    "            else:\n",
    "                tf_vector.append(0)\n",
    "        return Vectors.dense(tf_vector)\n",
    "\n",
    "    key_id = [i for i in range(len(tuples))]\n",
    "    key_values = {tuples[i][0]: i for i in key_id}\n",
    "    topWordsBC = sc.broadcast(dictionary.keys().collect())\n",
    "    feat = d_keyAndListOfWords.map(lambda x: (key_values[x[0]], TF(x[1], topWordsBC.value)))\n",
    "    labels = d_keyAndListOfWords.map(lambda x: (key_values[x[0]], int(x[0][0] == 'A' and x[0][1] == 'U')))\n",
    "    train_feat_df = sqlContext.createDataFrame(feat, ['ind', 'features'])\n",
    "    train_labels_df = sqlContext.createDataFrame(labels, ['ind', 'labels'])\n",
    "    train_df = train_feat_df.join(train_labels_df, on=['ind']).sort(['ind'])\n",
    "    train_df.cache()\n",
    "\n",
    "    svc = LinearSVC(labelCol='labels')\n",
    "    svg_model = svc.fit(train_df)\n",
    "\n",
    "    st_test_read = time.time()\n",
    "    t_corpus = sc.textFile('Google-Playstore_test.csv')\n",
    "    t_keyAndText = t_corpus.map(lambda x: (x[x.index('id=\"') + 4: x.index('\" url=')], x[x.index('\">') + 2:][:-6]))\n",
    "    regex = re.compile('[^a-zA-Z]')\n",
    "    t_keyAndListOfWords = t_keyAndText.map(lambda x: (str(x[0]), regex.sub(' ', x[1]).lower().split())).sortByKey(False)\n",
    "    t_tuples = t_keyAndListOfWords.collect()\n",
    "    t_key_id = [i for i in range(len(t_tuples))]\n",
    "    t_key_values = {t_tuples[i][0]: i for i in t_key_id}\n",
    "    t_feat = t_keyAndListOfWords.map(lambda x: (t_key_values[x[0]], TF(x[1], topWordsBC.value)))\n",
    "    t_labels = t_keyAndListOfWords.map(\n",
    "        lambda x: (t_key_values[x[0]], int(x[0][0] == 'A' and x[0][1] == 'U')))\n",
    "    test_feat_df = sqlContext.createDataFrame(t_feat, ['ind', 'features'])\n",
    "    test_labels_df = sqlContext.createDataFrame(t_labels, ['ind', 'labels'])\n",
    "    test_df = test_feat_df.join(test_labels_df, on=['ind']).sort(['ind'])\n",
    "    test_df.cache()\n",
    "\n",
    "    st_test = time.time()\n",
    "    test_pred = svg_model.evaluate(test_df).predictions\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol='labels')\n",
    "\n",
    "    print('Acc of Test: ', evaluator.evaluate(test_pred, {evaluator.metricName: \"accuracy\"}))\n",
    "    print('F1 of Test:', evaluator.evaluate(test_pred, {evaluator.metricName: \"f1\"}))\n",
    "    \n",
    "    end5 = time.time()\n",
    "\n",
    "    print(f\"Computation time of SVM regression by BGD is {(end5 - start5)/60} minutes\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
